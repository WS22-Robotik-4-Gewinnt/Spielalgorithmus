{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.86298583\n",
      "Iteration 2, loss = 0.62113627\n",
      "Iteration 3, loss = 0.58507849\n",
      "Iteration 4, loss = 0.56860185\n",
      "Iteration 5, loss = 0.55726038\n",
      "Iteration 6, loss = 0.54764096\n",
      "Iteration 7, loss = 0.53872318\n",
      "Iteration 8, loss = 0.53069612\n",
      "Iteration 9, loss = 0.52369928\n",
      "Iteration 10, loss = 0.51679232\n",
      "Iteration 11, loss = 0.51030496\n",
      "Iteration 12, loss = 0.50440629\n",
      "Iteration 13, loss = 0.49916431\n",
      "Iteration 14, loss = 0.49434510\n",
      "Iteration 15, loss = 0.48987095\n",
      "Iteration 16, loss = 0.48599049\n",
      "Iteration 17, loss = 0.48263210\n",
      "Iteration 18, loss = 0.47939368\n",
      "Iteration 19, loss = 0.47644288\n",
      "Iteration 20, loss = 0.47375694\n",
      "Iteration 21, loss = 0.47099389\n",
      "Iteration 22, loss = 0.46871423\n",
      "Iteration 23, loss = 0.46695501\n",
      "Iteration 24, loss = 0.46469321\n",
      "Iteration 25, loss = 0.46282204\n",
      "Iteration 26, loss = 0.46143608\n",
      "Iteration 27, loss = 0.45896809\n",
      "Iteration 28, loss = 0.45785087\n",
      "Iteration 29, loss = 0.45610190\n",
      "Iteration 30, loss = 0.45513750\n",
      "Iteration 31, loss = 0.45363150\n",
      "Iteration 32, loss = 0.45207582\n",
      "Iteration 33, loss = 0.45146172\n",
      "Iteration 34, loss = 0.44954452\n",
      "Iteration 35, loss = 0.44875646\n",
      "Iteration 36, loss = 0.44784842\n",
      "Iteration 37, loss = 0.44640369\n",
      "Iteration 38, loss = 0.44554707\n",
      "Iteration 39, loss = 0.44488919\n",
      "Iteration 40, loss = 0.44407703\n",
      "Iteration 41, loss = 0.44313676\n",
      "Iteration 42, loss = 0.44259664\n",
      "Iteration 43, loss = 0.44172661\n",
      "Iteration 44, loss = 0.44099206\n",
      "Iteration 45, loss = 0.44035139\n",
      "Iteration 46, loss = 0.44010135\n",
      "Iteration 47, loss = 0.43889707\n",
      "Iteration 48, loss = 0.43867020\n",
      "Iteration 49, loss = 0.43748260\n",
      "Iteration 50, loss = 0.43742738\n",
      "Iteration 51, loss = 0.43623053\n",
      "Iteration 52, loss = 0.43559132\n",
      "Iteration 53, loss = 0.43539050\n",
      "Iteration 54, loss = 0.43473193\n",
      "Iteration 55, loss = 0.43438864\n",
      "Iteration 56, loss = 0.43369378\n",
      "Iteration 57, loss = 0.43335820\n",
      "Iteration 58, loss = 0.43239258\n",
      "Iteration 59, loss = 0.43202510\n",
      "Iteration 60, loss = 0.43122383\n",
      "Iteration 61, loss = 0.43127203\n",
      "Iteration 62, loss = 0.43052561\n",
      "Iteration 63, loss = 0.43039369\n",
      "Iteration 64, loss = 0.42997914\n",
      "Iteration 65, loss = 0.42994114\n",
      "Iteration 66, loss = 0.42954739\n",
      "Iteration 67, loss = 0.42903851\n",
      "Iteration 68, loss = 0.42810167\n",
      "Iteration 69, loss = 0.42729575\n",
      "Iteration 70, loss = 0.42793808\n",
      "Iteration 71, loss = 0.42717316\n",
      "Iteration 72, loss = 0.42648184\n",
      "Iteration 73, loss = 0.42648287\n",
      "Iteration 74, loss = 0.42552612\n",
      "Iteration 75, loss = 0.42496189\n",
      "Iteration 76, loss = 0.42534334\n",
      "Iteration 77, loss = 0.42473426\n",
      "Iteration 78, loss = 0.42412187\n",
      "Iteration 79, loss = 0.42465332\n",
      "Iteration 80, loss = 0.42355473\n",
      "Iteration 81, loss = 0.42368216\n",
      "Iteration 82, loss = 0.42277960\n",
      "Iteration 83, loss = 0.42257387\n",
      "Iteration 84, loss = 0.42216066\n",
      "Iteration 85, loss = 0.42177550\n",
      "Iteration 86, loss = 0.42220054\n",
      "Iteration 87, loss = 0.42142689\n",
      "Iteration 88, loss = 0.42116221\n",
      "Iteration 89, loss = 0.42032152\n",
      "Iteration 90, loss = 0.42065056\n",
      "Iteration 91, loss = 0.42036760\n",
      "Iteration 92, loss = 0.41937603\n",
      "Iteration 93, loss = 0.41966015\n",
      "Iteration 94, loss = 0.41943699\n",
      "Iteration 95, loss = 0.41877235\n",
      "Iteration 96, loss = 0.41830996\n",
      "Iteration 97, loss = 0.41846482\n",
      "Iteration 98, loss = 0.41830017\n",
      "Iteration 99, loss = 0.41805067\n",
      "Iteration 100, loss = 0.41829198\n",
      "Iteration 101, loss = 0.41786215\n",
      "Iteration 102, loss = 0.41741202\n",
      "Iteration 103, loss = 0.41713096\n",
      "Iteration 104, loss = 0.41657631\n",
      "Iteration 105, loss = 0.41722714\n",
      "Iteration 106, loss = 0.41656870\n",
      "Iteration 107, loss = 0.41638589\n",
      "Iteration 108, loss = 0.41600610\n",
      "Iteration 109, loss = 0.41572096\n",
      "Iteration 110, loss = 0.41552023\n",
      "Iteration 111, loss = 0.41551581\n",
      "Iteration 112, loss = 0.41478744\n",
      "Iteration 113, loss = 0.41499670\n",
      "Iteration 114, loss = 0.41570041\n",
      "Iteration 115, loss = 0.41572264\n",
      "Iteration 116, loss = 0.41471193\n",
      "Iteration 117, loss = 0.41473864\n",
      "Iteration 118, loss = 0.41479650\n",
      "Iteration 119, loss = 0.41422890\n",
      "Iteration 120, loss = 0.41460577\n",
      "Iteration 121, loss = 0.41418464\n",
      "Iteration 122, loss = 0.41412125\n",
      "Iteration 123, loss = 0.41401922\n",
      "Iteration 124, loss = 0.41317311\n",
      "Iteration 125, loss = 0.41332406\n",
      "Iteration 126, loss = 0.41280794\n",
      "Iteration 127, loss = 0.41317138\n",
      "Iteration 128, loss = 0.41289921\n",
      "Iteration 129, loss = 0.41262632\n",
      "Iteration 130, loss = 0.41297909\n",
      "Iteration 131, loss = 0.41288239\n",
      "Iteration 132, loss = 0.41291583\n",
      "Iteration 133, loss = 0.41251824\n",
      "Iteration 134, loss = 0.41250384\n",
      "Iteration 135, loss = 0.41247159\n",
      "Iteration 136, loss = 0.41218820\n",
      "Iteration 137, loss = 0.41175913\n",
      "Iteration 138, loss = 0.41176359\n",
      "Iteration 139, loss = 0.41225533\n",
      "Iteration 140, loss = 0.41197835\n",
      "Iteration 141, loss = 0.41201438\n",
      "Iteration 142, loss = 0.41182648\n",
      "Iteration 143, loss = 0.41110184\n",
      "Iteration 144, loss = 0.41114055\n",
      "Iteration 145, loss = 0.41115291\n",
      "Iteration 146, loss = 0.41192684\n",
      "Iteration 147, loss = 0.41111449\n",
      "Iteration 148, loss = 0.41094045\n",
      "Iteration 149, loss = 0.41120933\n",
      "Iteration 150, loss = 0.41070495\n",
      "Iteration 151, loss = 0.41053365\n",
      "Iteration 152, loss = 0.41072162\n",
      "Iteration 153, loss = 0.41136445\n",
      "Iteration 154, loss = 0.41131305\n",
      "Iteration 155, loss = 0.41061847\n",
      "Iteration 156, loss = 0.41081885\n",
      "Iteration 157, loss = 0.41033452\n",
      "Iteration 158, loss = 0.41072284\n",
      "Iteration 159, loss = 0.41071527\n",
      "Iteration 160, loss = 0.41049026\n",
      "Iteration 161, loss = 0.41013983\n",
      "Iteration 162, loss = 0.41040044\n",
      "Iteration 163, loss = 0.41081644\n",
      "Iteration 164, loss = 0.41010322\n",
      "Iteration 165, loss = 0.40988512\n",
      "Iteration 166, loss = 0.41010929\n",
      "Iteration 167, loss = 0.40962691\n",
      "Iteration 168, loss = 0.41008635\n",
      "Iteration 169, loss = 0.41002009\n",
      "Iteration 170, loss = 0.40987443\n",
      "Iteration 171, loss = 0.40955011\n",
      "Iteration 172, loss = 0.41021573\n",
      "Iteration 173, loss = 0.40991936\n",
      "Iteration 174, loss = 0.40931509\n",
      "Iteration 175, loss = 0.40948196\n",
      "Iteration 176, loss = 0.40973214\n",
      "Iteration 177, loss = 0.40940341\n",
      "Iteration 178, loss = 0.40948973\n",
      "Iteration 179, loss = 0.40880040\n",
      "Iteration 180, loss = 0.40954774\n",
      "Iteration 181, loss = 0.40892352\n",
      "Iteration 182, loss = 0.40916074\n",
      "Iteration 183, loss = 0.40921274\n",
      "Iteration 184, loss = 0.40885242\n",
      "Iteration 185, loss = 0.40898158\n",
      "Iteration 186, loss = 0.40875250\n",
      "Iteration 187, loss = 0.40888032\n",
      "Iteration 188, loss = 0.40834440\n",
      "Iteration 189, loss = 0.40862857\n",
      "Iteration 190, loss = 0.40885978\n",
      "Iteration 191, loss = 0.40854437\n",
      "Iteration 192, loss = 0.40865251\n",
      "Iteration 193, loss = 0.40798929\n",
      "Iteration 194, loss = 0.40825333\n",
      "Iteration 195, loss = 0.40862832\n",
      "Iteration 196, loss = 0.40806412\n",
      "Iteration 197, loss = 0.40782490\n",
      "Iteration 198, loss = 0.40795744\n",
      "Iteration 199, loss = 0.40798206\n",
      "Iteration 200, loss = 0.40848301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8214920071047958\n",
      "Precision: 0.8702788141163886\n",
      "Recall: 0.8214920071047958\n",
      "F1 score: 0.8424197490860788\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # to load and manipulate data and for one-hot-encoding\n",
    "import numpy as np # to calculate the mean and standard deviation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('connect-4.data', header = None) # pandas (pd) reads dataset and returns a data frame (df)\n",
    "\n",
    "width = ['A', 'B', 'C', 'D', 'E', 'F', 'G'] # column names changed in A to G \n",
    "height = range(1, 7) # row number is added\n",
    "cols = []\n",
    "\n",
    "for i in width:\n",
    "    for j in height:\n",
    "        cols.append(i + str(j))\n",
    "cols.append('res')\n",
    "df.columns = cols\n",
    "\n",
    "# converting the characters x, b and o into numerical 1, 0 and -1\n",
    "df[df == 'x'] = 1 \n",
    "df[df == 'b'] = 0\n",
    "df[df == 'o'] = -1\n",
    "\n",
    "# splits the original data set into three separate data sets \"win\", \"loss\" and \"draw\" \n",
    "df_w = df[df['res'] == 'win']\n",
    "df_l = df[df['res'] == 'loss']\n",
    "df_d = df[df['res'] == 'draw']\n",
    "\n",
    "# calculates the number of games a player has won/loss/draw and stores it in the variable\n",
    "w_number = len(df_w)\n",
    "l_number = len(df_l)\n",
    "d_number = len(df_d)\n",
    "\n",
    "# divide datasets into training and test datasets\n",
    "# 70% of the dataset is used as the training dataset (sample)\n",
    "w_train = df_w.sample(n = (int)(w_number * 0.7))\n",
    "l_train = df_l.sample(n = (int)(l_number * 0.7))\n",
    "d_train = df_d.sample(n = (int)(d_number * 0.7))\n",
    "\n",
    "# 30% is used as the test dataset (drop)\n",
    "w_test = df_w.drop(w_train.index)\n",
    "l_test = df_l.drop(l_train.index)\n",
    "d_test = df_d.drop(d_train.index)\n",
    "\n",
    "# the resulting training and test datasets are merged into a single dataset\n",
    "train = pd.concat([w_train, l_train, d_train], axis = 0)\n",
    "test = pd.concat([w_test, l_test, d_test], axis = 0)\n",
    "\n",
    "# training the multilayer perceptron classification (MLP) model and predicting the results\n",
    "x_features = list(df.columns)\n",
    "y_feature = 'res'\n",
    "x_features.remove(y_feature)\n",
    "\n",
    "X_train = train[x_features].values\n",
    "X_test = test[x_features].values\n",
    "\n",
    "Y_train = train[y_feature].values\n",
    "Y_test = test[y_feature].values\n",
    "\n",
    "MLP_clf = MLPClassifier(hidden_layer_sizes = (25, 10), verbose = 1)\n",
    "MLP_clf.fit(X_train, Y_train)\n",
    "\n",
    "prediction = MLP_clf.predict(X_test)\n",
    "MLP_clf.score(X_test, Y_test)\n",
    "\n",
    "# prediction = Y_predict[0][2] - Y_predict[0][0]\n",
    "\n",
    "# evaluate classifier\n",
    "accuracy = accuracy_score(prediction, Y_test)\n",
    "precision = precision_score(prediction, Y_test, average = 'weighted')\n",
    "recall = recall_score(prediction, Y_test, average = 'weighted')\n",
    "f1 = f1_score(prediction, Y_test, average = 'weighted')\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1 score: \" + str(f1))\n",
    "\n",
    "# serialize and save model\n",
    "filename = 'MLP_clf.data'\n",
    "pickle.dump(MLP_clf, open(filename, 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f5e823d87b14054c1dc00802eb14baf1948661152c3a5fae825f19f31ac0e35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
